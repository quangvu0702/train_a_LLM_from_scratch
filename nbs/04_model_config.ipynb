{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)\n",
    "\n",
    "# Model is randomly initialized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.37.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub Copilot: This configuration is for a BERT model. Here's what each parameter means:\n",
    "\n",
    "# - `\"attention_probs_dropout_prob\": 0.1`: The dropout probability for the attention probabilities.\n",
    "# - `\"classifier_dropout\": null`: The dropout probability for the classifier. `null` means it's not set.\n",
    "# - `\"hidden_act\": \"gelu\"`: The non-linear activation function in the encoder and pooler layers. Here, it's Gaussian Error Linear Units (GELU).\n",
    "# - `\"hidden_dropout_prob\": 0.1`: The dropout probability for all fully connected layers in the embeddings and encoder.\n",
    "# - `\"hidden_size\": 768`: The size of the \"hidden\" layer in the Transformer encoder.\n",
    "# - `\"initializer_range\": 0.02`: The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
    "# - `\"intermediate_size\": 3072`: The size of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n",
    "# - `\"layer_norm_eps\": 1e-12`: The epsilon to use in the layer normalization layers.\n",
    "# - `\"max_position_embeddings\": 512`: The maximum sequence length that this model might ever be used with.\n",
    "# - `\"model_type\": \"bert\"`: The type of the model.\n",
    "# - `\"num_attention_heads\": 12`: The number of attention heads for each attention layer in the Transformer encoder.\n",
    "# - `\"num_hidden_layers\": 12`: The number of hidden layers in the Transformer encoder.\n",
    "# - `\"pad_token_id\": 0`: The id of the padding token.\n",
    "# - `\"position_embedding_type\": \"absolute\"`: The type of position embeddings.\n",
    "# - `\"transformers_version\": \"4.37.1\"`: The version of the transformers library.\n",
    "# - `\"type_vocab_size\": 2`: The vocabulary size of the `token_type_ids` passed when calling `BertModel` or `TFBertModel`.\n",
    "# - `\"use_cache\": true`: Whether or not the model uses the past key/values to speed up decoding.\n",
    "# - `\"vocab_size\": 30522`: The vocabulary size of the `inputs_ids` passed when calling `BertModel` or `TFBertModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dea45d4a9944e3eacd40dec85071b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00d67dc372541a08ad20fbc282d2ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# config.json, pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save model\n",
    "# model.save_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;36mconfig.json\u001b[0m@  \u001b[01;36mmodel.safetensors\u001b[0m@\n"
     ]
    }
   ],
   "source": [
    "ls  ~/.cache/huggingface/hub/models--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samwell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
