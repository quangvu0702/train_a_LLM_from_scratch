{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create byte pair encoding (BPE) for a given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Taylor Ali', b'Taylor Ali', 84, 97, 'T', 'a')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_text =\"\"\"Taylor Alison Swift (born December 13, 1989) is an American singer-songwriter. Her artistry, songwriting, and entrepreneurship have influenced the music industry and popular culture. A subject of widespread media coverage, Swift is an advocate of artists' rights and has impacted politics.\n",
    "Swift began professional songwriting at age 14. She signed with Big Machine Records in 2005 and achieved prominence as a country pop singer with the albums Taylor Swift (2006) and Fearless (2008). Their singles \"Teardrops on My Guitar\", \"Love Story\", and \"You Belong with Me\" were crossover successes on country and pop radio formats and brought Swift mainstream fame. She experimented with rock and electronic styles on her next albums, Speak Now (2010) and Red (2012), respectively, with the latter featuring her first Billboard Hot 100 number-one single, \"We Are Never Ever Getting Back Together\". Swift recalibrated her image from country to pop with 1989 (2014), a synth-pop album containing the chart-topping songs \"Shake It Off\", \"Blank Space\", and \"Bad Blood\". Media scrutiny inspired the hip-hop-influenced Reputation (2017) and its number-one single \"Look What You Made Me Do\".\n",
    "After signing with Republic Records in 2018, Swift released the eclectic pop album Lover (2019) and the autobiographical documentary Miss Americana (2020). She explored indie folk styles on the 2020 albums Folklore and Evermore, subdued electropop on Midnights (2022), and re-recorded four albums subtitled Taylor's Version[a] after a dispute with Big Machine. These albums spawned the number-one songs \"Cruel Summer\", \"Cardigan\", \"Willow\", \"Anti-Hero\", \"All Too Well\", and \"Is It Over Now?\". Her Eras Tour (2023â€“2024) and its accompanying concert film became the highest-grossing tour and concert film of all time, respectively. Swift has directed videos and films such as Folklore: The Long Pond Studio Sessions (2020) and All Too Well: The Short Film (2021).\n",
    "One of the world's best-selling musicians, Swift has sold over 200 million records as of 2019. She is the highest-grossing female touring act, the most-streamed woman on Spotify and Apple Music, and the first billionaire with music as the main source of income. Six of her albums have opened with over one million sales in a week. The 2023 Time Person of the Year, Swift has appeared on lists such as Rolling Stone's 100 Greatest Songwriters of All Time, Billboard's Greatest of All Time Artists, and Forbes' World's 100 Most Powerful Women. Her accolades include 14 Grammy Awards, a Primetime Emmy Award, 40 American Music Awards, 40 Billboard Music Awards, and 23 MTV Video Music Awards; she has won the Grammy Award for Album of the Year, the MTV Video Music Award for Video of the Year, and the IFPI Global Recording Artist of the Year a record four times each.'\n",
    "\"\"\"\n",
    "\n",
    "# convert to bytes\n",
    "xs = origin_text.encode('utf-8')\n",
    "\n",
    "origin_text[0:10], xs[0:10], xs[0], xs[1], chr(xs[0]), chr(xs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = xs.decode('utf-8')\n",
    "xs == origin_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_2_token = {i: chr(i) for i in range(256)} # index to token\n",
    "n_vocab = 276\n",
    "merged_bytes = {} # index to token\n",
    "n_extend_token = 276 - len(index_2_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_2_token[97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 99, 9, 1]\n",
      "2809\n",
      "2195\n"
     ]
    }
   ],
   "source": [
    "# count frequency of each pair of bytes and get highest frequency pair\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_stats(xs, pair_freq=None):\n",
    "    if pair_freq is None: pair_freq = defaultdict(int)\n",
    "    for x1, x2 in zip(xs, xs[1:]):\n",
    "        pair = (x1, x2)\n",
    "        pair_freq[pair] += 1\n",
    "    return pair_freq\n",
    "\n",
    "token = lambda idx: index_2_token[idx]\n",
    "\n",
    "def merge(xs, pair, new_token_id):\n",
    "    i = 0\n",
    "    new_xs = []\n",
    "    while i < len(xs):\n",
    "        if (i < len(xs) - 1) and ((xs[i], xs[i+1]) == pair):\n",
    "            new_xs.append(new_token_id)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_xs.append(xs[i])\n",
    "            i += 1\n",
    "    return new_xs\n",
    "\n",
    "print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))\n",
    "\n",
    "def train_tokenizer():\n",
    "    xs = origin_text.encode('utf-8') # convert str to list of index\n",
    "    xs = list(xs) # convert to list of index\n",
    "    # print length of xs\n",
    "    print(len(xs))\n",
    "    for i in range(n_extend_token):\n",
    "        next_index = len(index_2_token)\n",
    "        pair_freq = get_stats(xs) # get common index pair\n",
    "        _, pair = max([(v, k) for k, v in pair_freq.items()])\n",
    "        merged_bytes[pair] = next_index\n",
    "        index_2_token[next_index] = token(pair[0]) + token(pair[1])\n",
    "        xs = merge(xs, pair, next_index)\n",
    "    print(len(xs))\n",
    "    \n",
    "train_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 256, byte_pair (101, 32), 'e'+' '='e '\n",
      "index 257, byte_pair (32, 97), ' '+'a'=' a'\n",
      "index 258, byte_pair (100, 32), 'd'+' '='d '\n",
      "index 259, byte_pair (115, 32), 's'+' '='s '\n",
      "index 260, byte_pair (101, 114), 'e'+'r'='er'\n",
      "index 261, byte_pair (105, 110), 'i'+'n'='in'\n",
      "index 262, byte_pair (111, 110), 'o'+'n'='on'\n",
      "index 263, byte_pair (116, 104), 't'+'h'='th'\n",
      "index 264, byte_pair (257, 110), ' a'+'n'=' an'\n",
      "index 265, byte_pair (116, 32), 't'+' '='t '\n",
      "index 266, byte_pair (264, 258), ' an'+'d '=' and '\n",
      "index 267, byte_pair (44, 32), ','+' '=', '\n",
      "index 268, byte_pair (263, 256), 'th'+'e '='the '\n",
      "index 269, byte_pair (114, 101), 'r'+'e'='re'\n",
      "index 270, byte_pair (50, 48), '2'+'0'='20'\n",
      "index 271, byte_pair (119, 105), 'w'+'i'='wi'\n",
      "index 272, byte_pair (97, 114), 'a'+'r'='ar'\n",
      "index 273, byte_pair (261, 103), 'in'+'g'='ing'\n",
      "index 274, byte_pair (111, 114), 'o'+'r'='or'\n",
      "index 275, byte_pair (108, 108), 'l'+'l'='ll'\n"
     ]
    }
   ],
   "source": [
    "# vocab\n",
    "for byte_pair, merge_idx in merged_bytes.items():\n",
    "    print(f\"index {merge_idx}, byte_pair {byte_pair}, '{index_2_token[byte_pair[0]]}'+'{index_2_token[byte_pair[1]]}'='{index_2_token[merge_idx]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 101, 108, 108, 111, 44, 32, 119, 111, 114, 108, 100, 33, 49, 50, 51]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from list of index to list of index\n",
    "# find each pair of index and replace with new index\n",
    "\n",
    "\n",
    "def encode(text):\n",
    "    xs = list(text.encode('utf-8'))\n",
    "    while len(xs) >= 2:\n",
    "        pair_freq = get_stats(xs)\n",
    "        pair_freq = [(merged_bytes.get(pair, float('inf')), pair) for _, pair in pair_freq]\n",
    "        idx, pair = min(pair_freq)\n",
    "        if idx == float('inf'): break\n",
    "        xs = merge(xs, pair, idx)\n",
    "    return xs\n",
    "\n",
    "text = \"Hello, world!123\"\n",
    "encoded_text = encode(text)\n",
    "encoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, world!123'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert each index into token\n",
    "def decode(xs):\n",
    "    return ''.join([index_2_token[x] for x in xs])\n",
    "\n",
    "decoded_text = decode(encoded_text)\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "text = \"Marshall Bruce Mathers III, known professionally as Eminem, is an American rapper. He is credited with popularizing hip hop in Middle America and is often regarded as one of the greatest rappers of all time.\"\n",
    "encoded_text = encode(text)\n",
    "decoded_text = decode(encoded_text)\n",
    "text == decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using chatGTP 2 style tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for i in range(1, 101):\n",
      "    if i % 3 == 0 and i % 5 == 0:\n",
      "        print(\"FizzBuzz\")\n",
      "    elif i % 3 == 0:\n",
      "        print(\"Fizz\")\n",
      "    elif i % 5 == 0:\n",
      "        print(\"Buzz\")\n",
      "    else:\n",
      "        print(i)\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "# This line of code is a regular expression (regex) pattern used to match certain types of strings in a larger body of text.\n",
    "# Here's a breakdown of what each part of the pattern does:\n",
    "\n",
    "# - `'s|'t|'re|'ve|'m|'ll|'d`: This matches contractions like 's, 't, 're, 've, 'm, 'll, and 'd. The pipe character `|` means \"or\", so the pattern matches any one of these contractions.\n",
    "\n",
    "# - ` ?\\p{L}+`: This matches one or more Unicode letters, possibly preceded by a space. `\\p{L}` is a Unicode property escape that matches any kind of letter from any language. The `?` means \"zero or one of the preceding element\", so a space is optional before the letter. The `+` means \"one or more of the preceding element\".\n",
    "\n",
    "# - ` ?\\p{N}+`: This matches one or more Unicode numbers, possibly preceded by a space. `\\p{N}` is a Unicode property escape that matches any kind of numeric character in any script.\n",
    "\n",
    "# - ` ?[^\\s\\p{L}\\p{N}]+`: This matches one or more characters that are not whitespace, letters, or numbers, possibly preceded by a space. The `^` inside the square brackets negates the character set, so `[^\\s\\p{L}\\p{N}]` matches any character that is not a whitespace character (`\\s`), a letter (`\\p{L}`), or a number (`\\p{N}`).\n",
    "\n",
    "# - `\\s+(?!\\S)|\\s+`: This matches one or more whitespace characters. The `(?!\\S)` is a negative lookahead that asserts that what immediately follows the current position in the string is not a non-whitespace character.\n",
    "\n",
    "# This pattern is typically used with the `re.findall()` function to split a string into tokens. The tokens can include contractions, words, numbers, punctuation, and whitespace.\n",
    "\n",
    "text = \"\"\"for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\"\"\"\n",
    "words = pat.findall(text)\n",
    "\n",
    "encoded_text = sum([encode(w) for w in words], [])\n",
    "decoded_text = decode(encoded_text)\n",
    "assert text == decoded_text\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 256, byte_pair (115, 32), 's'+' '='s '\n",
      "index 257, byte_pair (110, 32), 'n'+' '='n '\n",
      "index 258, byte_pair (101, 114), 'e'+'r'='er'\n",
      "index 259, byte_pair (101, 32), 'e'+' '='e '\n",
      "index 260, byte_pair (111, 102), 'o'+'f'='of'\n",
      "index 261, byte_pair (32, 97), ' '+'a'=' a'\n",
      "index 262, byte_pair (116, 104), 't'+'h'='th'\n",
      "index 263, byte_pair (116, 101), 't'+'e'='te'\n",
      "index 264, byte_pair (114, 101), 'r'+'e'='re'\n",
      "index 265, byte_pair (108, 108), 'l'+'l'='ll'\n",
      "index 266, byte_pair (105, 256), 'i'+'s '='is '\n",
      "index 267, byte_pair (97, 114), 'a'+'r'='ar'\n",
      "index 268, byte_pair (265, 32), 'll'+' '='ll '\n",
      "index 269, byte_pair (261, 256), ' a'+'s '=' as '\n",
      "index 270, byte_pair (258, 256), 'er'+'s '='ers '\n",
      "index 271, byte_pair (258, 105), 'er'+'i'='eri'\n",
      "index 272, byte_pair (271, 99), 'eri'+'c'='eric'\n",
      "index 273, byte_pair (272, 97), 'eric'+'a'='erica'\n",
      "index 274, byte_pair (114, 97), 'r'+'a'='ra'\n",
      "index 275, byte_pair (274, 112), 'ra'+'p'='rap'\n"
     ]
    }
   ],
   "source": [
    "# put all together\n",
    "token = lambda idx: index_2_token[idx]\n",
    "\n",
    "class MyTokenizer:\n",
    "    def __init__(self, vocab_size=276):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.index_2_token = {i: chr(i) for i in range(256)} # index to token\n",
    "        self.merged_bytes = {} # index to token\n",
    "        self.n_extend_token = self.vocab_size - len(self.index_2_token)\n",
    "\n",
    "    def token(self, idx): return self.index_2_token[idx]\n",
    "\n",
    "    def train_tokenizer(self, train_text):\n",
    "        xs = train_text.encode('utf-8') # convert str to list of index\n",
    "        xs = list(xs) # convert to list of index\n",
    "        # print length of xs\n",
    "        # print(f\"original length {len(xs)}\")\n",
    "        for i in range(self.n_extend_token):\n",
    "            next_index = len(self.index_2_token)\n",
    "            stats_pair = get_stats(xs) # get common index pair\n",
    "            _, pair = max([(v, k) for k, v in stats_pair.items()])\n",
    "            self.merged_bytes[pair] = next_index\n",
    "            self.index_2_token[next_index] = self.token(pair[0]) + self.token(pair[1])\n",
    "            xs = merge(xs, pair, next_index)\n",
    "        # print(f\"length after tokenize {len(xs)}\")\n",
    "\n",
    "    def print_vocab(self):\n",
    "        for byte_pair, merge_idx in self.merged_bytes.items():\n",
    "            print(f\"index {merge_idx}, byte_pair {byte_pair}, '{self.index_2_token[byte_pair[0]]}'+'{self.index_2_token[byte_pair[1]]}'='{self.index_2_token[merge_idx]}'\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        xs = list(text.encode('utf-8'))\n",
    "        while len(xs) >= 2:\n",
    "            pair_freq = get_stats(xs)\n",
    "            pair_freq = [(self.merged_bytes.get(pair, float('inf')), pair) for _, pair in pair_freq]\n",
    "            idx, pair = min(pair_freq)\n",
    "            if idx == float('inf'): break\n",
    "            xs = merge(xs, pair, idx)\n",
    "        return xs\n",
    "\n",
    "    def decode(self, xs):\n",
    "        return ''.join([self.index_2_token[x] for x in xs])\n",
    "    \n",
    "\n",
    "\n",
    "tokenizer = MyTokenizer()\n",
    "\n",
    "text = \"Marshall Bruce Mathers III, known professionally as Eminem, is an American rapper. He is credited with popularizing hip hop in Middle America and is often regarded as one of the greatest rappers of all time.\"\n",
    "tokenizer.train_tokenizer(text)\n",
    "\n",
    "tokenizer.print_vocab()\n",
    "\n",
    "\n",
    "encoded_text = tokenizer.encode(text)\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "assert text == decoded_text\n",
    "\n",
    "decoded_text\n",
    "text = \"\"\"for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\"\"\"\n",
    "encoded_text = tokenizer.encode(text)\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "assert text == decoded_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " ',',\n",
       " ' I',\n",
       " \"'m\",\n",
       " ' a',\n",
       " ' student',\n",
       " '.',\n",
       " ' I',\n",
       " \"'m\",\n",
       " ' learning',\n",
       " ' NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Split text before tokenization\n",
    "import regex as re\n",
    "\n",
    "pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "words = pat.findall(\"Hi, I'm a student. I'm learning NLP.\")\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {('a', 'b'): 2, ('b', ' '): 1, (' ', 'a'): 1})"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = get_stats('ab ab')\n",
    "pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {('a', 'b'): 2, (' ', 'a'): 1})"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = get_stats('ab')\n",
    "pair = get_stats(' ab', pair)\n",
    "pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 256, byte_pair (101, 114), 'e'+'r'='er'\n",
      "index 257, byte_pair (32, 97), ' '+'a'=' a'\n",
      "index 258, byte_pair (111, 102), 'o'+'f'='of'\n",
      "index 259, byte_pair (32, 105), ' '+'i'=' i'\n",
      "index 260, byte_pair (259, 115), ' i'+'s'=' is'\n",
      "index 261, byte_pair (116, 104), 't'+'h'='th'\n",
      "index 262, byte_pair (116, 101), 't'+'e'='te'\n",
      "index 263, byte_pair (114, 101), 'r'+'e'='re'\n",
      "index 264, byte_pair (108, 108), 'l'+'l'='ll'\n",
      "index 265, byte_pair (97, 114), 'a'+'r'='ar'\n",
      "index 266, byte_pair (32, 258), ' '+'of'=' of'\n",
      "index 267, byte_pair (257, 115), ' a'+'s'=' as'\n",
      "index 268, byte_pair (257, 110), ' a'+'n'=' an'\n",
      "index 269, byte_pair (256, 115), 'er'+'s'='ers'\n",
      "index 270, byte_pair (256, 105), 'er'+'i'='eri'\n",
      "index 271, byte_pair (270, 99), 'eri'+'c'='eric'\n",
      "index 272, byte_pair (271, 97), 'eric'+'a'='erica'\n",
      "index 273, byte_pair (114, 97), 'r'+'a'='ra'\n",
      "index 274, byte_pair (273, 112), 'ra'+'p'='rap'\n",
      "index 275, byte_pair (274, 112), 'rap'+'p'='rapp'\n"
     ]
    }
   ],
   "source": [
    "# put all together\n",
    "token = lambda idx: index_2_token[idx]\n",
    "\n",
    "class MyTokenizer:\n",
    "    def __init__(self, vocab_size=276):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.index_2_token = {i: chr(i) for i in range(256)} # index to token\n",
    "        self.merged_bytes = {} # index to token\n",
    "        self.n_extend_token = self.vocab_size - len(self.index_2_token)\n",
    "\n",
    "    def token(self, idx): return self.index_2_token[idx]\n",
    "\n",
    "    def train_tokenizer(self, train_text):\n",
    "        words = pat.findall(train_text)\n",
    "        xs = [list(w.encode('utf-8')) for w in words] # convert str to list of index\n",
    "        for i in range(self.n_extend_token):\n",
    "            next_index = len(self.index_2_token)\n",
    "            stats_pair = None\n",
    "            for x in xs: stats_pair = get_stats(x, stats_pair)\n",
    "            _, pair = max([(v, k) for k, v in stats_pair.items()])\n",
    "            self.merged_bytes[pair] = next_index\n",
    "            self.index_2_token[next_index] = self.token(pair[0]) + self.token(pair[1])\n",
    "            xs = [merge(x, pair, next_index) for x in xs]\n",
    "        # print(f\"length after tokenize {len(xs)}\")\n",
    "\n",
    "    def print_vocab(self):\n",
    "        for byte_pair, merge_idx in self.merged_bytes.items():\n",
    "            print(f\"index {merge_idx}, byte_pair {byte_pair}, '{self.index_2_token[byte_pair[0]]}'+'{self.index_2_token[byte_pair[1]]}'='{self.index_2_token[merge_idx]}'\")\n",
    "\n",
    "    def _encode(self, word):\n",
    "        xs = list(word.encode('utf-8'))\n",
    "        while len(xs) >= 2:\n",
    "            pair_freq = get_stats(xs)\n",
    "            pair_freq = [(self.merged_bytes.get(pair, float('inf')), pair) for _, pair in pair_freq]\n",
    "            idx, pair = min(pair_freq)\n",
    "            if idx == float('inf'): break\n",
    "            xs = merge(xs, pair, idx)\n",
    "        return xs\n",
    "\n",
    "    def encode(self, text):\n",
    "        words = pat.findall(text)\n",
    "        return sum([self._encode(w) for w in words], [])\n",
    "\n",
    "    def decode(self, xs):\n",
    "        return ''.join([self.index_2_token[x] for x in xs])\n",
    "    \n",
    "\n",
    "\n",
    "tokenizer = MyTokenizer()\n",
    "\n",
    "text = \"Marshall Bruce Mathers III, known professionally as Eminem, is an American rapper. He is credited with popularizing hip hop in Middle America and is often regarded as one of the greatest rappers of all time.\"\n",
    "tokenizer.train_tokenizer(text)\n",
    "\n",
    "tokenizer.print_vocab()\n",
    "\n",
    "\n",
    "encoded_text = tokenizer.encode(text)\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "assert text == decoded_text\n",
    "\n",
    "# decoded_text\n",
    "text = \"\"\"for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\"\"\"\n",
    "encoded_text = tokenizer.encode(text)\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "assert text == decoded_text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samwell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
